{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3eb40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.7-cp310-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.6 (from PyMuPDF)\n",
      "  Using cached PyMuPDFb-1.24.6-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.7-cp310-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 3.2/3.2 MB 3.9 MB/s eta 0:00:00\n",
      "Using cached PyMuPDFb-1.24.6-py3-none-win_amd64.whl (12.5 MB)\n",
      "Installing collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.24.7 PyMuPDFb-1.24.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d980b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark     \n",
      "Python    \n",
      "MySQL\n",
      "8359918992\n",
      "linkedin.com/in/abhayjr11\n",
      "TECHNICAL SKILLS\n",
      "PROJECTS\n",
      "abhayjaiswal@gmail.com.com\n",
      "github.com/abhayjr11\n",
      "Snowflake\n",
      "Almost 2 years of experience with Data Engineering and Big Data technologies, My Internship with Bigdata helped me to explore the Bigdata technologies, and worked on a healthcare project with my team using Hadoop, Python, MySQL, Pandas, spark, hive, Continuously Practicing Python, MySQL, Pandas, ETL, Data Pipeline, Started Learning Apache Spark and Scala, Working on small projects to sharpen my technical skills. I Learning Enthuses can grasp more and more with time and is flexible to work with Data.  \n",
      "Hadoop                                  \n",
      "WORK EXPERIENCE: \n",
      "Training: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\n",
      "Built basic ETL that ingested transactional and event data from an Insurance company with 12,000 users, Transformed to a Local Database and Loaded to HDFS HIVE Tables.\n",
      "Worked with clients to understand business needs and translate those business needs into actionable Data, saving 17 hours of manual work each week.\n",
      "Used Spark in Python to distribute data processing on large Batch datasets, improving ingestion and speed by 67%\n",
      "Worked as Project Manager, Project Flow Control, and Team Management using Jira, Successfully Delivered the Report to the client on time.\n",
      "Training: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\n",
      "Maintained data pipeline while ingesting batch and transactional data across 8 different primary data s\n",
      "ources using Python\n",
      "Used Python, and SQL, and collaborated with 2 interns to create a cloud-first data ingestion that improve\n",
      "d the    processing speed of data by 74%\n",
      "Extracted data from 7 disparate sources, and increased agility and accuracy with a centralized system.\n",
      "      Data Analytics using MySQL and Python, Visualization using Seaborn and Matplotlib \n",
      "COGNIZANT - DATA SCIENCE INTERN (BIG DATA) \n",
      "JANUARY 2022 - JUNE 2022 | PUNE, MAHARASHTRA \n",
      "Apache Spark     \n",
      "Python    \n",
      "MySQL\n",
      " C++  \n",
      "JAVA\n",
      "ETL\n",
      "         MySQL\n",
      "         Snowflake\n",
      "         Seaborn\n",
      "         Hadoop                                  \n",
      "         Data Visualization\n",
      "Data Modeling\n",
      "Cloud Computing\n",
      "Big Dat a\n",
      "ODBC\n",
      "MapReduce\n",
      "Matplotlib\n",
      "AWS\n",
      "Data Warehouse\n",
      "Data Visualization\n",
      "  HEALTHCARE SYSTEM BIG DATA ANALYTICS\n",
      "         April 2022 - May 2022\n",
      "         Environment: Linux (Ubuntu 18.04)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read and display the resume\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf_document:\n",
    "            for page_num in range(len(pdf_document)):\n",
    "                page = pdf_document.load_page(page_num)\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "pdf_file = \"abhay_resume_2024.pdf\"  # Replace with the actual path to your PDF file\n",
    "pdf_text = read_pdf(pdf_file)\n",
    "\n",
    "print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7886fcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in the directory:\n",
      ".ipynb_checkpoints\n",
      "abhayjaiswal.pdf\n",
      "abhay_resume_2024.pdf\n",
      "resumeupdated.pdf\n",
      "Untitled.ipynb\n",
      "\n",
      "PDF files in the directory:\n",
      "abhayjaiswal.pdf\n",
      "abhay_resume_2024.pdf\n",
      "resumeupdated.pdf\n"
     ]
    }
   ],
   "source": [
    "# read and display multiple resume \n",
    "\n",
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = \"C:\\\\Users\\\\HP\\\\Desktop\\\\resume\"\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = os.listdir(directory_path)\n",
    "\n",
    "# Optionally, filter for specific file types, e.g., PDF files\n",
    "pdf_files = [f for f in all_files if f.endswith('.pdf')]\n",
    "\n",
    "print(\"All files in the directory:\")\n",
    "for file in all_files:\n",
    "    print(file)\n",
    "\n",
    "print(\"\\nPDF files in the directory:\")\n",
    "for pdf_file in pdf_files:\n",
    "    print(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94160450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf():\n",
    "        pdf_files=[]\n",
    "        # Specify the directory path\n",
    "        directory_path = \"C:\\\\Users\\\\HP\\\\Desktop\\\\resume\"\n",
    "\n",
    "        # Get a list of all files in the directory\n",
    "        all_files = os.listdir(directory_path)\n",
    "\n",
    "        # Optionally, filter for specific file types, e.g., PDF files\n",
    "        pdf_files = [f for f in all_files if f.endswith('.pdf')]\n",
    "        for pdf_file in pdf_files:\n",
    "            pdf_files.append(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7889d727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abhayjaiswal.pdf', 'abhay_resume_2024.pdf', 'resumeupdated.pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2786d5ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_pdf() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Iterate through the list of PDF files, read their content, and append it to pdf_contents\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pdf_file \u001b[38;5;129;01min\u001b[39;00m pdf_files:\n\u001b[1;32m---> 23\u001b[0m     pdf_text \u001b[38;5;241m=\u001b[39m \u001b[43mread_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     pdf_contents\u001b[38;5;241m.\u001b[39mappend(pdf_text)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Now pdf_contents contains the text from each PDF file\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_pdf() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
