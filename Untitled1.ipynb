{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d93955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def read_pdf(pdf):\n",
    "    text = \"\"\n",
    "    path = \"C:\\\\Users\\\\HP\\\\Desktop\\\\resume\"\n",
    "    file_path=path+'\\\\'+'abhayjaiswal.pdf'\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf_document:\n",
    "            for page_num in range(len(pdf_document)):\n",
    "                page = pdf_document.load_page(page_num)\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# List your PDF files here\n",
    "pdf_files = ['abhayjaiswal.pdf', 'abhay_resume_2024.pdf', 'resumeupdated.pdf']\n",
    "\n",
    "# Initialize a list to store the contents of each PDF\n",
    "pdf_contents = []\n",
    "\n",
    "# Iterate through the list of PDF files, read their content, and append it to pdf_contents\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_text = read_pdf(pdf_file)\n",
    "    pdf_contents.append(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2468fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df8012ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_n_save():\n",
    "    text = \"\"\n",
    "    # Initialize a list to store the contents of each PDF\n",
    "    pdf_contents = []\n",
    "    path= \"C:\\\\Users\\\\HP\\\\Desktop\\\\resume\" # folder path\n",
    "    pdf_file =\"\"  # actual path of file\n",
    "\n",
    "  # list of pdf files fetch from folder\n",
    "    pdf_files = ['abhayjaiswal.pdf', 'abhay_resume_2024.pdf', 'resumeupdated.pdf']\n",
    "    for f in pdf_files:\n",
    "        file_path = path+\"\\\\\"+f        \n",
    "        # Iterate through the list of PDF files, read their content, and append it to pdf_contents      \n",
    "        try:\n",
    "            with fitz.open(file_path) as pdf_document:\n",
    "                for page_num in range(len(pdf_document)):\n",
    "                    page = pdf_document.load_page(page_num)\n",
    "                    text += page.get_text()\n",
    "                pdf_contents.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"file has not any text\")\n",
    "                \n",
    "\n",
    "    print(pdf_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d724953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark     Learning    \\nScala      Learning \\nPython\\n C++  \\nMySQL\\nTECHNICAL SKILLS:\\nPROJECTS :\\nProficient in:      \\nETLs, ODBC\\nSQL - MySQL, Hive\\nSnowflake\\nSeaborn, Matplotlib \\nHadoop, MapReduce                                   \\nWORK EXPERIENCE: \\nABHAY JAISWAL\\n8359918992\\nabhay.jaiswal2@cognizant.com\\nlinkedin.com/in/abhayjr11\\ngithub.com/abhayjr11\\nJaiswal House , Vidhya Vihar Colony, Bus Stand, Bargi, Jabalpur (MP)- 482051\\nProgramming Languages :                                    \\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nBuilt basic ETL that ingested transactional and event data from an Insurance company with\\n12,000 users, Transform to Local Database, Load to HDFS HIVE Tables.\\nWorked with client to understand business needs and translate those business needs into\\nactionable Data, saving 17 hours of manual work each week.\\nUsed Spark in Python to distribute data processing on large Batch datasets, improving\\ningestion and speed by 67%\\nWorked as Project Manager, Project Flow Control and Team Management using Jira,\\nSuccessfully Delivered the Report to the client on time.\\nCOGNIZANT - DATA SCIENCE INTERN (BIG DATA) \\nJANUARY 2022 - JUNE 2022 | PUNE, MAHARASHTRA \\nCOGNIZANT - PROGRAMMER ANALYST (DATA ENGINEER)\\nJUNE 2022 - CURRENT | PUNE, MAHARASHTRA \\nMaintained data pipeline while ingesting batch and transactional data across 8 different\\nprimary data sources using Python\\nUsed Python, SQL, collaborate with 2 interns to create a cloud-first data ingestion that\\nimproved processing speed of data by 74%\\nExtracted data from 7 disparate sources, and increased agility and accuracy with a\\ncentralized system.\\nData Analytics using MySQL and Python, Visualization using Seaborn and Matplotlib \\nProjects and Hands-On:\\n  HELTHCARE SYSTEM BIG DATA ANALYTICS\\n         April 2022 - May 2022\\n         Environment: Linux (Ubuntu 18.04)\\n         Hadoop 2.7.2\\n         Spark 2.0.2\\n         Sqoop 1.4.7\\n         Python3\\nSUMMARY :\\nAlmost 2 year of experience with Data Engineering and Bigdata technologies, Internship with\\nBigdata track helps me to explore the Bigdata technologies and worked on a healthcare project\\nwith my team using Hadoop, Python, MySQL, Pandas, spark, hive , Continually Practicing\\nPython, MySQL, Pandas, ETL, Data Pipeline, Started Learning Apache Spark and Scala, \\nWorking on small projects to sharpen my technical skills. I Learning Enthuses can grasp more\\nand more with time and flexible to work with Data.  \\n', 'Spark     Learning    \\nScala      Learning \\nPython\\n C++  \\nMySQL\\nTECHNICAL SKILLS:\\nPROJECTS :\\nProficient in:      \\nETLs, ODBC\\nSQL - MySQL, Hive\\nSnowflake\\nSeaborn, Matplotlib \\nHadoop, MapReduce                                   \\nWORK EXPERIENCE: \\nABHAY JAISWAL\\n8359918992\\nabhay.jaiswal2@cognizant.com\\nlinkedin.com/in/abhayjr11\\ngithub.com/abhayjr11\\nJaiswal House , Vidhya Vihar Colony, Bus Stand, Bargi, Jabalpur (MP)- 482051\\nProgramming Languages :                                    \\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nBuilt basic ETL that ingested transactional and event data from an Insurance company with\\n12,000 users, Transform to Local Database, Load to HDFS HIVE Tables.\\nWorked with client to understand business needs and translate those business needs into\\nactionable Data, saving 17 hours of manual work each week.\\nUsed Spark in Python to distribute data processing on large Batch datasets, improving\\ningestion and speed by 67%\\nWorked as Project Manager, Project Flow Control and Team Management using Jira,\\nSuccessfully Delivered the Report to the client on time.\\nCOGNIZANT - DATA SCIENCE INTERN (BIG DATA) \\nJANUARY 2022 - JUNE 2022 | PUNE, MAHARASHTRA \\nCOGNIZANT - PROGRAMMER ANALYST (DATA ENGINEER)\\nJUNE 2022 - CURRENT | PUNE, MAHARASHTRA \\nMaintained data pipeline while ingesting batch and transactional data across 8 different\\nprimary data sources using Python\\nUsed Python, SQL, collaborate with 2 interns to create a cloud-first data ingestion that\\nimproved processing speed of data by 74%\\nExtracted data from 7 disparate sources, and increased agility and accuracy with a\\ncentralized system.\\nData Analytics using MySQL and Python, Visualization using Seaborn and Matplotlib \\nProjects and Hands-On:\\n  HELTHCARE SYSTEM BIG DATA ANALYTICS\\n         April 2022 - May 2022\\n         Environment: Linux (Ubuntu 18.04)\\n         Hadoop 2.7.2\\n         Spark 2.0.2\\n         Sqoop 1.4.7\\n         Python3\\nSUMMARY :\\nAlmost 2 year of experience with Data Engineering and Bigdata technologies, Internship with\\nBigdata track helps me to explore the Bigdata technologies and worked on a healthcare project\\nwith my team using Hadoop, Python, MySQL, Pandas, spark, hive , Continually Practicing\\nPython, MySQL, Pandas, ETL, Data Pipeline, Started Learning Apache Spark and Scala, \\nWorking on small projects to sharpen my technical skills. I Learning Enthuses can grasp more\\nand more with time and flexible to work with Data.  \\nApache Spark     \\nPython    \\nMySQL\\n8359918992\\nlinkedin.com/in/abhayjr11\\nTECHNICAL SKILLS\\nPROJECTS\\nabhayjaiswal@gmail.com.com\\ngithub.com/abhayjr11\\nSnowflake\\nAlmost 2 years of experience with Data Engineering and Big Data technologies, My Internship with Bigdata helped me to explore the Bigdata technologies, and worked on a healthcare project with my team using Hadoop, Python, MySQL, Pandas, spark, hive, Continuously Practicing Python, MySQL, Pandas, ETL, Data Pipeline, Started Learning Apache Spark and Scala, Working on small projects to sharpen my technical skills. I Learning Enthuses can grasp more and more with time and is flexible to work with Data.  \\nHadoop                                  \\nWORK EXPERIENCE: \\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nBuilt basic ETL that ingested transactional and event data from an Insurance company with 12,000 users, Transformed to a Local Database and Loaded to HDFS HIVE Tables.\\nWorked with clients to understand business needs and translate those business needs into actionable Data, saving 17 hours of manual work each week.\\nUsed Spark in Python to distribute data processing on large Batch datasets, improving ingestion and speed by 67%\\nWorked as Project Manager, Project Flow Control, and Team Management using Jira, Successfully Delivered the Report to the client on time.\\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nMaintained data pipeline while ingesting batch and transactional data across 8 different primary data s\\nources using Python\\nUsed Python, and SQL, and collaborated with 2 interns to create a cloud-first data ingestion that improve\\nd the    processing speed of data by 74%\\nExtracted data from 7 disparate sources, and increased agility and accuracy with a centralized system.\\n      Data Analytics using MySQL and Python, Visualization using Seaborn and Matplotlib \\nCOGNIZANT - DATA SCIENCE INTERN (BIG DATA) \\nJANUARY 2022 - JUNE 2022 | PUNE, MAHARASHTRA \\nApache Spark     \\nPython    \\nMySQL\\n C++  \\nJAVA\\nETL\\n         MySQL\\n         Snowflake\\n         Seaborn\\n         Hadoop                                  \\n         Data Visualization\\nData Modeling\\nCloud Computing\\nBig Dat a\\nODBC\\nMapReduce\\nMatplotlib\\nAWS\\nData Warehouse\\nData Visualization\\n  HEALTHCARE SYSTEM BIG DATA ANALYTICS\\n         April 2022 - May 2022\\n         Environment: Linux (Ubuntu 18.04)\\n', 'Spark     Learning    \\nScala      Learning \\nPython\\n C++  \\nMySQL\\nTECHNICAL SKILLS:\\nPROJECTS :\\nProficient in:      \\nETLs, ODBC\\nSQL - MySQL, Hive\\nSnowflake\\nSeaborn, Matplotlib \\nHadoop, MapReduce                                   \\nWORK EXPERIENCE: \\nABHAY JAISWAL\\n8359918992\\nabhay.jaiswal2@cognizant.com\\nlinkedin.com/in/abhayjr11\\ngithub.com/abhayjr11\\nJaiswal House , Vidhya Vihar Colony, Bus Stand, Bargi, Jabalpur (MP)- 482051\\nProgramming Languages :                                    \\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nBuilt basic ETL that ingested transactional and event data from an Insurance company with\\n12,000 users, Transform to Local Database, Load to HDFS HIVE Tables.\\nWorked with client to understand business needs and translate those business needs into\\nactionable Data, saving 17 hours of manual work each week.\\nUsed Spark in Python to distribute data processing on large Batch datasets, improving\\ningestion and speed by 67%\\nWorked as Project Manager, Project Flow Control and Team Management using Jira,\\nSuccessfully Delivered the Report to the client on time.\\nCOGNIZANT - DATA SCIENCE INTERN (BIG DATA) \\nJANUARY 2022 - JUNE 2022 | PUNE, MAHARASHTRA \\nCOGNIZANT - PROGRAMMER ANALYST (DATA ENGINEER)\\nJUNE 2022 - CURRENT | PUNE, MAHARASHTRA \\nMaintained data pipeline while ingesting batch and transactional data across 8 different\\nprimary data sources using Python\\nUsed Python, SQL, collaborate with 2 interns to create a cloud-first data ingestion that\\nimproved processing speed of data by 74%\\nExtracted data from 7 disparate sources, and increased agility and accuracy with a\\ncentralized system.\\nData Analytics using MySQL and Python, Visualization using Seaborn and Matplotlib \\nProjects and Hands-On:\\n  HELTHCARE SYSTEM BIG DATA ANALYTICS\\n         April 2022 - May 2022\\n         Environment: Linux (Ubuntu 18.04)\\n         Hadoop 2.7.2\\n         Spark 2.0.2\\n         Sqoop 1.4.7\\n         Python3\\nSUMMARY :\\nAlmost 2 year of experience with Data Engineering and Bigdata technologies, Internship with\\nBigdata track helps me to explore the Bigdata technologies and worked on a healthcare project\\nwith my team using Hadoop, Python, MySQL, Pandas, spark, hive , Continually Practicing\\nPython, MySQL, Pandas, ETL, Data Pipeline, Started Learning Apache Spark and Scala, \\nWorking on small projects to sharpen my technical skills. I Learning Enthuses can grasp more\\nand more with time and flexible to work with Data.  \\nApache Spark     \\nPython    \\nMySQL\\n8359918992\\nlinkedin.com/in/abhayjr11\\nTECHNICAL SKILLS\\nPROJECTS\\nabhayjaiswal@gmail.com.com\\ngithub.com/abhayjr11\\nSnowflake\\nAlmost 2 years of experience with Data Engineering and Big Data technologies, My Internship with Bigdata helped me to explore the Bigdata technologies, and worked on a healthcare project with my team using Hadoop, Python, MySQL, Pandas, spark, hive, Continuously Practicing Python, MySQL, Pandas, ETL, Data Pipeline, Started Learning Apache Spark and Scala, Working on small projects to sharpen my technical skills. I Learning Enthuses can grasp more and more with time and is flexible to work with Data.  \\nHadoop                                  \\nWORK EXPERIENCE: \\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nBuilt basic ETL that ingested transactional and event data from an Insurance company with 12,000 users, Transformed to a Local Database and Loaded to HDFS HIVE Tables.\\nWorked with clients to understand business needs and translate those business needs into actionable Data, saving 17 hours of manual work each week.\\nUsed Spark in Python to distribute data processing on large Batch datasets, improving ingestion and speed by 67%\\nWorked as Project Manager, Project Flow Control, and Team Management using Jira, Successfully Delivered the Report to the client on time.\\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nMaintained data pipeline while ingesting batch and transactional data across 8 different primary data s\\nources using Python\\nUsed Python, and SQL, and collaborated with 2 interns to create a cloud-first data ingestion that improve\\nd the    processing speed of data by 74%\\nExtracted data from 7 disparate sources, and increased agility and accuracy with a centralized system.\\n      Data Analytics using MySQL and Python, Visualization using Seaborn and Matplotlib \\nCOGNIZANT - DATA SCIENCE INTERN (BIG DATA) \\nJANUARY 2022 - JUNE 2022 | PUNE, MAHARASHTRA \\nApache Spark     \\nPython    \\nMySQL\\n C++  \\nJAVA\\nETL\\n         MySQL\\n         Snowflake\\n         Seaborn\\n         Hadoop                                  \\n         Data Visualization\\nData Modeling\\nCloud Computing\\nBig Dat a\\nODBC\\nMapReduce\\nMatplotlib\\nAWS\\nData Warehouse\\nData Visualization\\n  HEALTHCARE SYSTEM BIG DATA ANALYTICS\\n         April 2022 - May 2022\\n         Environment: Linux (Ubuntu 18.04)\\nSpark  \\nC++\\nPython\\nJAVA\\nMySQL\\nTECHNICAL SKILLS:\\nPROJECTS :\\nEDUCATION :\\nProficient in:      \\nETLs, ODBC\\nSQL - MySQL, Hive\\nSnowflake\\nSeaborn, Matplotlib \\nAWS(EMR, Sagemaker, Lambda, Kinesis)\\nHadoop, MapReduce                                   \\nWORK EXPERIENCE: \\nABHAY JAISWAL\\n8359918992\\nabhayjaiswal901@gmail.com\\nlinkedin.com/in/abhayjr11\\ngithub.com/abhayjr11\\nJaiswal House , Vidhya Vihar Colony, Bus Stand, Bargi, Jabalpur (MP)- 482051\\nProgramming Languages :                                    \\nTraining: BigData, Hadoop environment, Hive, Spark, Python, Data Warehousing, MySQL\\nBuilt basic ETL that ingested transactional and event data from an Insurance company with\\n12,000 users, Transform to Local Database, Load to HDFS HIVE Tables.\\nWorked with client to understand business needs and translate those business needs into\\nactionable Data, saving 17 hours of manual work each week.\\nUsed Spark in Python to distribute data processing on large Batch datasets, improving\\ningestion and speed by 67%\\nWorked as Project Manager, Project Flow Control and Team Management using Jira,\\nSuccessfully Delivered the Report to the client on time.\\nCOGNIZANT - DATA SCIENCE INTERN (BIG DATA) \\nJANUARY 2022 - JUNE 2022 | PUNE, MAHARASHTRA \\nCOGNIZANT - PROGRAMMER ANALYST (DATA ENGINEER)\\nJUNE 2022 - CURRENT | PUNE, MAHARASHTRA \\nMaintained data pipeline up-time of 99.8%, while ingesting streaming and transactional\\ndata across 8 different primary data sources using Spark, Redshift, S3, and Python\\nUsed Python, SQL, and Spark collaborate with 2 interns and a junior data engineer to\\ncreate a cloud-first data ingestion that improved processing speed of data by 74%\\nExtracted data from 7 disparate sources, and increased agility and accuracy with a\\ncentralized system.\\nData Analytics using MySQL and Apache Spark, Visualization using Seaborn and\\nMatplotlib \\nGYAN GANGA COLLEGE OF TECHNOLOGY, JABALPUR\\n         2020 - 2022 \\n         Master of Computer Application(MCA)\\nGOVT. MODEL SCIENCE COLLEGE, JABALPUR\\n         2016 - 2019 \\n         Bachelor of Computer Application(BCA)\\nJAWAHAR NAVODAYA VIDYALAYA, BARGI NAGAR, JABALPUR\\n         2016 - 2019 \\n         Higher Secondary Studies (10+2)\\nProjects and Hands-On:\\n  HELTHCARE SYSTEM BIG DATA ANALYTICS\\n         April 2022 - May 2022\\n         Environment: Linux (Ubuntu 18.04)\\n         Hadoop 2.7.2\\n         Spark 2.0.2\\n         Sqoop 1.4.7\\n         Python3\\n • HACKATHON 1.0 2021:  \\n      PRO-GROW\\n      May 2021\\n      Technology used: React, Python flask,     \\n       MySQL\\n      Platform: Android Application\\n']\n"
     ]
    }
   ],
   "source": [
    "read_n_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb346e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
